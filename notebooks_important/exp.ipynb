{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "    Doc\n",
    ")\n",
    "segmenter = Segmenter()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "\n",
    "### ----------------------------- key sentences block -----------------------------\n",
    "\n",
    "def find_synax_tokens_with_order(doc, start, tokens, text_arr, full_str):\n",
    "    ''' Находит все синтаксические токены, соответствующие заданному набору простых токенов (найденные\n",
    "        для определенной NER другими функциями).\n",
    "        Возвращает словарь найденных синтаксических токенов (ключ - идентификатор токена, состоящий\n",
    "        из номера предложения и номера токена внутри предложения).\n",
    "        Начинает поиск с указанной позиции в списке синтаксических токенов, дополнительно возвращает\n",
    "        позицию остановки, с которой нужно продолжить поиск следующей NER.\n",
    "    '''\n",
    "    found = []\n",
    "    in_str = False\n",
    "    str_candidate = ''\n",
    "    str_counter = 0\n",
    "    if len(text_arr) == 0:\n",
    "        return [], start\n",
    "    for i in range(start, len(doc.syntax.tokens)):\n",
    "        t = doc.syntax.tokens[i]\n",
    "        if in_str:\n",
    "            str_counter += 1\n",
    "            if str_counter < len(text_arr) and t.text == text_arr[str_counter]:\n",
    "                str_candidate += t.text\n",
    "                found.append(t)\n",
    "                if str_candidate == full_str:\n",
    "                    return found, i+1\n",
    "            else:\n",
    "                in_str = False\n",
    "                str_candidate = ''\n",
    "                str_counter = 0\n",
    "                found = []\n",
    "        if t.text == text_arr[0]:\n",
    "            found.append(t)\n",
    "            str_candidate = t.text\n",
    "            if str_candidate == full_str:\n",
    "                return found, i+1\n",
    "            in_str = True\n",
    "    return [], len(doc.syntax.tokens)\n",
    "\n",
    "\n",
    "def find_tokens_in_diap_with_order(doc, start_token, diap):\n",
    "    ''' Находит все простые токены (без синтаксической информации), которые попадают в\n",
    "        указанный диапазон. Эти диапазоны мы получаем из разметки NER.\n",
    "        Возвращает набор найденных токенов и в виде массива токенов, и в виде массива строчек.\n",
    "        Начинает поиск с указанной позиции в строке и дополнительно возвращает позицию остановки.\n",
    "    '''\n",
    "    found_tokens = []\n",
    "    found_text = []\n",
    "    full_str = ''\n",
    "    next_i = 0\n",
    "    for i in range(start_token, len(doc.tokens)):\n",
    "        t = doc.tokens[i]\n",
    "        if t.start > diap[-1]:\n",
    "            next_i = i\n",
    "            break\n",
    "        if t.start in diap:\n",
    "            found_tokens.append(t)\n",
    "            found_text.append(t.text)\n",
    "            full_str += t.text\n",
    "    return found_tokens, found_text, full_str, next_i\n",
    "\n",
    "\n",
    "def add_found_arr_to_dict(found, dict_dest):\n",
    "    for synt in found:\n",
    "        dict_dest.update({synt.id: synt})\n",
    "    return dict_dest\n",
    "\n",
    "\n",
    "def make_all_syntax_dict(doc):\n",
    "    all_syntax = {}\n",
    "    for synt in doc.syntax.tokens:\n",
    "        all_syntax.update({synt.id: synt})\n",
    "    return all_syntax\n",
    "\n",
    "\n",
    "def is_consiquent(id_1, id_2):\n",
    "    ''' Проверяет идут ли токены друг за другом без промежутка по ключам. '''\n",
    "    id_1_list = id_1.split('_')\n",
    "    id_2_list = id_2.split('_')\n",
    "    if id_1_list[0] != id_2_list[0]:\n",
    "        return False\n",
    "    return int(id_1_list[1]) + 1 == int(id_2_list[1])\n",
    "\n",
    "\n",
    "def replace_found_to(found, x_str):\n",
    "    ''' Заменяет последовательность токенов NER на «заглушку». '''\n",
    "    prev_id = '0_0'\n",
    "    for synt in found:\n",
    "        if is_consiquent(prev_id, synt.id):\n",
    "            synt.text = ''\n",
    "        else:\n",
    "            synt.text = x_str\n",
    "        prev_id = synt.id\n",
    "\n",
    "\n",
    "def analyze_doc(text):\n",
    "    ''' Запускает Natasha для анализа документа. '''\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    doc.parse_syntax(syntax_parser)\n",
    "    ner_tagger = NewsNERTagger(emb)\n",
    "    doc.tag_ner(ner_tagger)\n",
    "    return doc\n",
    "\n",
    "\n",
    "def find_non_sym_syntax_short(entity_name, doc, add_X=False, x_str='X'):\n",
    "    ''' Отыскивает заданную сущность в тексте, среди всех NER (возможно, в другой грамматической форме).\n",
    "\n",
    "        entity_name - сущность, которую ищем;\n",
    "        doc - документ, в котором сделан препроцессинг Natasha;\n",
    "        add_X - сделать ли замену сущности на «заглушку»;\n",
    "        x_str - текст замены.\n",
    "\n",
    "        Возвращает:\n",
    "        all_found_syntax - словарь всех подходящих токенов образующих искомые сущности, в котором\n",
    "        в случае надобности произведена замена NER на «заглушку»;\n",
    "        all_syntax - словарь всех токенов.\n",
    "    '''\n",
    "    all_found_syntax = {}\n",
    "    current_synt_number = 0\n",
    "    current_tok_number = 0\n",
    "\n",
    "    # идем по всем найденным NER\n",
    "    for span in doc.spans:\n",
    "        span.normalize(morph_vocab)\n",
    "        if span.type != 'ORG':\n",
    "            continue\n",
    "        diap = range(span.start, span.stop)\n",
    "        # создаем словарь всех синтаксических элементов (ключ -- id из номера предложения и номера внутри предложения)\n",
    "        all_syntax = make_all_syntax_dict(doc)\n",
    "        # находим все простые токены внутри NER\n",
    "        found_tokens, found_text, full_str, current_tok_number = find_tokens_in_diap_with_order(doc, current_tok_number,\n",
    "                                                                                                diap)\n",
    "        # по найденным простым токенам находим все синтаксические токены внутри данного NER\n",
    "        found, current_synt_number = find_synax_tokens_with_order(doc, current_synt_number, found_tokens, found_text,\n",
    "                                                                  full_str)\n",
    "        # если текст NER совпадает с указанной сущностью, то делаем замену\n",
    "        if entity_name.find(span.normal) >= 0 or span.normal.find(entity_name) >= 0:\n",
    "            if add_X:\n",
    "                replace_found_to(found, x_str)\n",
    "            all_found_syntax = add_found_arr_to_dict(found, all_found_syntax)\n",
    "    return all_found_syntax, all_syntax\n",
    "\n",
    "\n",
    "def key_sentences(all_found_syntax):\n",
    "    ''' Находит номера предложений с искомой NER. '''\n",
    "    key_sent_numb = {}\n",
    "    for synt in all_found_syntax.keys():\n",
    "        key_sent_numb.update({synt.split('_')[0]: 1})\n",
    "    return key_sent_numb\n",
    "\n",
    "\n",
    "def openinig_punct(x):\n",
    "    opennings = ['«', '(']\n",
    "    return x in opennings\n",
    "\n",
    "\n",
    "def key_sentences_str(entitiy_name, doc, add_X=False, x_str='X', return_all=True):\n",
    "    ''' Составляет окончательный текст, в котором есть только предложения, где есть ключевая сущность,\n",
    "        эта сущность, если указано, заменяется на «заглушку».\n",
    "    '''\n",
    "    all_found_syntax, all_syntax = find_non_sym_syntax_short(entitiy_name, doc, add_X, x_str)\n",
    "    key_sent_numb = key_sentences(all_found_syntax)\n",
    "    str_ret = ''\n",
    "\n",
    "    for s in all_syntax.keys():\n",
    "        if (s.split('_')[0] in key_sent_numb.keys()) or (return_all):\n",
    "            to_add = all_syntax[s]\n",
    "\n",
    "            if s in all_found_syntax.keys():\n",
    "                to_add = all_found_syntax[s]\n",
    "            else:\n",
    "                if to_add.rel == 'punct' and not openinig_punct(to_add.text):\n",
    "                    str_ret = str_ret.rstrip()\n",
    "\n",
    "            str_ret += to_add.text\n",
    "            if (not openinig_punct(to_add.text)) and (to_add.text != ''):\n",
    "                str_ret += ' '\n",
    "\n",
    "    return str_ret\n",
    "\n",
    "\n",
    "### ----------------------------- key entities block -----------------------------\n",
    "\n",
    "\n",
    "def find_synt(doc, synt_id):\n",
    "    for synt in doc.syntax.tokens:\n",
    "        if synt.id == synt_id:\n",
    "            return synt\n",
    "    return None\n",
    "\n",
    "\n",
    "def is_subj(doc, synt, recursion_list=[]):\n",
    "    ''' Сообщает является ли слово подлежащим или частью сложного подлежащего. '''\n",
    "    if synt.rel == 'nsubj':\n",
    "        return True\n",
    "    if synt.rel == 'appos':\n",
    "        found_head = find_synt(doc, synt.head_id)\n",
    "        if found_head.id in recursion_list:\n",
    "            return False\n",
    "        return is_subj(doc, found_head, recursion_list + [synt.id])\n",
    "    return False\n",
    "\n",
    "\n",
    "def find_subjects_in_syntax(doc):\n",
    "    ''' Выдает словарик, в котором для каждой NER написано, является ли он\n",
    "        подлежащим в предложении.\n",
    "        Выдает стартовую позицию NER и было ли оно подлежащим (или appos)\n",
    "    '''\n",
    "    found_subjects = {}\n",
    "    current_synt_number = 0\n",
    "    current_tok_number = 0\n",
    "\n",
    "    for span in doc.spans:\n",
    "        span.normalize(morph_vocab)\n",
    "        if span.type != 'ORG':\n",
    "            continue\n",
    "\n",
    "        found_subjects.update({span.start: 0})\n",
    "        diap = range(span.start, span.stop)\n",
    "\n",
    "        found_tokens, found_text, full_str, current_tok_number = find_tokens_in_diap_with_order(doc,\n",
    "                                                                                                current_tok_number,\n",
    "                                                                                                diap)\n",
    "\n",
    "        found, current_synt_number = find_synax_tokens_with_order(doc, current_synt_number, found_tokens,\n",
    "                                                                  found_text, full_str)\n",
    "\n",
    "        found_subjects.update({span.start: 0})\n",
    "        for synt in found:\n",
    "            if is_subj(doc, synt):\n",
    "                found_subjects.update({span.start: 1})\n",
    "    return found_subjects\n",
    "\n",
    "\n",
    "def entity_weight(lst, c=1):\n",
    "    return c*lst[0]+lst[1]\n",
    "\n",
    "\n",
    "def determine_subject(found_subjects, doc, new_agency_list, return_best=True, threshold=0.75):\n",
    "    ''' Определяет ключевую NER и список самых важных NER, основываясь на том, сколько\n",
    "        раз каждая из них встречается в текста вообще и сколько раз в роли подлежащего '''\n",
    "    objects_arr = []\n",
    "    objects_arr_ners = []\n",
    "    should_continue = False\n",
    "    for span in doc.spans:\n",
    "        should_continue = False\n",
    "        span.normalize(morph_vocab)\n",
    "        if span.type != 'ORG':\n",
    "            continue\n",
    "        if span.normal in new_agency_list:\n",
    "            continue\n",
    "        for i in range(len(objects_arr)):\n",
    "            t, lst = objects_arr[i]\n",
    "\n",
    "            if t.find(span.normal) >= 0:\n",
    "                lst[0] += 1\n",
    "                lst[1] += found_subjects[span.start]\n",
    "                should_continue = True\n",
    "                break\n",
    "\n",
    "            if span.normal.find(t) >= 0:\n",
    "                objects_arr[i] = (span.normal, [lst[0]+1, lst[1]+found_subjects[span.start]])\n",
    "                should_continue = True\n",
    "                break\n",
    "\n",
    "        if should_continue:\n",
    "            continue\n",
    "        objects_arr.append((span.normal, [1, found_subjects[span.start]]))\n",
    "        objects_arr_ners.append(span.normal)\n",
    "\n",
    "    max_weight = 0\n",
    "    opt_ent = 0\n",
    "    for obj in objects_arr:\n",
    "        t, lst = obj\n",
    "        w = entity_weight(lst)\n",
    "        if max_weight < w:\n",
    "            max_weight = w\n",
    "            opt_ent = t\n",
    "\n",
    "    if not return_best:\n",
    "        return opt_ent, objects_arr_ners\n",
    "\n",
    "    bests = []\n",
    "    for obj in objects_arr:\n",
    "        t, lst = obj\n",
    "        w = entity_weight(lst)\n",
    "        if max_weight*threshold < w:\n",
    "            bests.append(t)\n",
    "\n",
    "    return opt_ent, bests\n",
    "\n",
    "\n",
    "text = '''В офисах Сбера начали тестировать технологию помощи посетителям в экстренных ситуациях. «Зеленая кнопка» будет\n",
    " в зонах круглосуточного обслуживания офисов банка в Воронеже, Санкт-Петербурге, Подольске, Пскове, Орле и Ярославле.\n",
    " В них находятся стенды с сенсорными кнопками, обеспечивающие связь с операторами центра мониторинга службы безопасности\n",
    " банка. Получив сигнал о помощи, оператор центра может подключиться к объекту по голосовой связи. С помощью камер\n",
    " видеонаблюдения он оценит обстановку и при необходимости вызовет полицию или скорую помощь. «Зеленой кнопкой» можно\n",
    " воспользоваться в нерабочее для отделения время, если возникла угроза жизни или здоровью. В остальных случаях помочь\n",
    " клиентам готовы сотрудники отделения банка. «Одно из направлений нашей работы в области ESG и устойчивого развития\n",
    " — это забота об обществе. И здоровье людей как высшая ценность является его основой. Поэтому задача банка в области\n",
    " безопасности гораздо масштабнее, чем обеспечение только финансовой безопасности клиентов. Этот пилотный проект\n",
    " приурочен к 180-летию Сбербанка: мы хотим, чтобы, приходя в банк, клиент чувствовал, что его жизнь и безопасность\n",
    " — наша ценность», — отметил заместитель председателя правления Сбербанка Станислав Кузнецов.'''\n",
    "\n",
    "doc = analyze_doc(text)\n",
    "key_entity = determine_subject(find_subjects_in_syntax(doc), doc, [])[0]\n",
    "text_for_model = key_sentences_str(key_entity, doc, add_X=True, x_str='X', return_all=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Doc(text='В офисах Сбера начали тестировать технологию помо..., tokens=[...], spans=[...], sents=[...])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Сбербанк'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'В офисах X начали тестировать технологию помощи посетителям в экстренных ситуациях. Этот пилотный проект приурочен к 180-летию X: мы хотим, чтобы, приходя в банк, клиент чувствовал, что его жизнь и безопасность— наша ценность»,— отметил заместитель председателя правления X Станислав Кузнецов. '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_for_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Прогнозы и комментарии. Коррекция или разворот?  Индекс МосБиржи от двухлетних максимумов ушел в коррекцию. Недельная разворотная свеча может настораживать краткосрочных активных трейдеров, а для инвесторов это возможность усилить позиции в подешевевших активах на перспективу — годовому тренду вверх вряд ли пока что-то угрожает.  Рубль за неделю потерял процент, индекс ОФЗ рухнул на минимумы апреля 2022 г. — валютные барьеры ограждают нацвалюту от девальвации и одновременно давят на котировки облигаций. Через неделю заседание ЦБ по ставке, и картина на валютном и долговом рынках прояснится.  Бумаги в фокусе — ТКС Холдинг и Росбанк, «префы» Мечела, Татнефти и Сургутнефтегаза.  На внешнем контуре умеренная коррекция в индексах США, утренние фьючерсы слабы, азиатские площадки окрашены в ярко красный — факторы указывают на негативный старт европейской пятничной сессии акций.  Оцениваем ближайшие перспективы в утреннем материале: https://bcs-express.ru/novosti-i-analitika/prognozy-i-kommentarii-korrektsiia-ili-razvorot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Сбер и Газпромыч, Сбер, GMKN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = analyze_doc(text)\n",
    "# key_entity = determine_subject(find_subjects_in_syntax(doc), doc, [])[0]\n",
    "# text_for_model = key_sentences_str(key_entity, doc, add_X=True, x_str='X', return_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocSpan(stop=4, type='LOC', text='Сбер', tokens=[...]),\n",
       " DocSpan(start=7, stop=16, type='LOC', text='Газпромыч', tokens=[...]),\n",
       " DocSpan(start=18, stop=22, type='LOC', text='Сбер', tokens=[...])]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Doc(text='Прогнозы и комментарии. Коррекция или разворот?  ..., tokens=[...], spans=[...], sents=[...])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocSent(stop=23, text='Прогнозы и комментарии.', tokens=[...]),\n",
       " DocSent(start=24, stop=47, text='Коррекция или разворот?', tokens=[...]),\n",
       " DocSent(start=49, stop=107, text='Индекс МосБиржи от двухлетних максимумов ушел в к..., tokens=[...]),\n",
       " DocSent(start=108, stop=330, text='Недельная разворотная свеча может настораживать к..., tokens=[...]),\n",
       " DocSent(start=332, stop=508, text='Рубль за неделю потерял процент, индекс ОФЗ рухну..., tokens=[...]),\n",
       " DocSent(start=509, stop=597, text='Через неделю заседание ЦБ по ставке, и картина на..., tokens=[...], spans=[...]),\n",
       " DocSent(start=599, stop=683, text='Бумаги в фокусе — ТКС Холдинг и Росбанк, «префы» ..., tokens=[...], spans=[...]),\n",
       " DocSent(start=685, stop=883, text='На внешнем контуре умеренная коррекция в индексах..., tokens=[...], spans=[...]),\n",
       " DocSent(start=885, stop=1029, text='Оцениваем ближайшие перспективы в утреннем матери..., tokens=[...])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocSpan(start=532, stop=534, type='ORG', text='ЦБ', tokens=[...], normal='ЦБ'),\n",
       " DocSpan(start=617, stop=628, type='ORG', text='ТКС Холдинг', tokens=[...], normal='ТКС Холдинг'),\n",
       " DocSpan(start=631, stop=638, type='ORG', text='Росбанк', tokens=[...], normal='Росбанк'),\n",
       " DocSpan(start=648, stop=654, type='ORG', text='Мечела', tokens=[...], normal='Мечела'),\n",
       " DocSpan(start=656, stop=664, type='ORG', text='Татнефти', tokens=[...], normal='Татнефть'),\n",
       " DocSpan(start=667, stop=682, type='ORG', text='Сургутнефтегаза', tokens=[...], normal='Сургутнефтегаза'),\n",
       " DocSpan(start=735, stop=738, type='LOC', text='США', tokens=[...], normal='США')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocSpan(start=532, stop=534, type='ORG', text='ЦБ', tokens=[...], normal='ЦБ'),\n",
       " DocSpan(start=617, stop=628, type='ORG', text='ТКС Холдинг', tokens=[...], normal='ТКС Холдинг'),\n",
       " DocSpan(start=631, stop=638, type='ORG', text='Росбанк', tokens=[...], normal='Росбанк'),\n",
       " DocSpan(start=648, stop=654, type='ORG', text='Мечела', tokens=[...], normal='Мечела'),\n",
       " DocSpan(start=656, stop=664, type='ORG', text='Татнефти', tokens=[...], normal='Татнефть'),\n",
       " DocSpan(start=667, stop=682, type='ORG', text='Сургутнефтегаза', tokens=[...], normal='Сургутнефтегаза'),\n",
       " DocSpan(start=735, stop=738, type='LOC', text='США', tokens=[...], normal='США')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ТКС Холдинг'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.spans[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Бумаги в фокусе — ТКС Холдинг и Росбанк, «префы» Мечела, Татнефти и Сургутнефтегаза.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_sent = doc.sents[6].start\n",
    "one_text = doc.sents[6].text\n",
    "one_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, stop = doc.spans[1].start, doc.spans[1].stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "617"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Бумаги в фокусе — X и Росбанк, «префы» Мечела, Татнефти и Сургутнефтегаза.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_one_text = one_text[:start-start_sent] + 'X' + one_text[stop-start_sent:]\n",
    "new_one_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocToken(start=617, stop=620, text='ТКС', id='7_5', head_id='7_3', rel='parataxis', pos='PROPN', feats=<Yes>),\n",
       " DocToken(start=621, stop=628, text='Холдинг', id='7_6', head_id='7_5', rel='appos', pos='PROPN', feats=<Inan,Nom,Masc,Sing>)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.spans[1].tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocSent(stop=23, text='Прогнозы и комментарии.', tokens=[...]),\n",
       " DocSent(start=24, stop=47, text='Коррекция или разворот?', tokens=[...]),\n",
       " DocSent(start=49, stop=107, text='Индекс МосБиржи от двухлетних максимумов ушел в к..., tokens=[...]),\n",
       " DocSent(start=108, stop=330, text='Недельная разворотная свеча может настораживать к..., tokens=[...]),\n",
       " DocSent(start=332, stop=508, text='Рубль за неделю потерял процент, индекс ОФЗ рухну..., tokens=[...]),\n",
       " DocSent(start=509, stop=597, text='Через неделю заседание ЦБ по ставке, и картина на..., tokens=[...], spans=[...]),\n",
       " DocSent(start=599, stop=683, text='Бумаги в фокусе — ТКС Холдинг и Росбанк, «префы» ..., tokens=[...], spans=[...]),\n",
       " DocSent(start=685, stop=883, text='На внешнем контуре умеренная коррекция в индексах..., tokens=[...], spans=[...]),\n",
       " DocSent(start=885, stop=1029, text='Оцениваем ближайшие перспективы в утреннем матери..., tokens=[...])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocToken(start=599, stop=605, text='Бумаги', id='7_1', head_id='7_3', rel='nsubj', pos='NOUN', feats=<Inan,Nom,Fem,Plur>),\n",
       " DocToken(start=606, stop=607, text='в', id='7_2', head_id='7_3', rel='case', pos='ADP'),\n",
       " DocToken(start=608, stop=614, text='фокусе', id='7_3', head_id='7_1', rel='nmod', pos='NOUN', feats=<Inan,Loc,Masc,Sing>),\n",
       " DocToken(start=615, stop=616, text='—', id='7_4', head_id='7_5', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=617, stop=620, text='ТКС', id='7_5', head_id='7_3', rel='parataxis', pos='PROPN', feats=<Yes>),\n",
       " DocToken(start=621, stop=628, text='Холдинг', id='7_6', head_id='7_5', rel='appos', pos='PROPN', feats=<Inan,Nom,Masc,Sing>),\n",
       " DocToken(start=629, stop=630, text='и', id='7_7', head_id='7_8', rel='cc', pos='CCONJ'),\n",
       " DocToken(start=631, stop=638, text='Росбанк', id='7_8', head_id='7_5', rel='conj', pos='PROPN', feats=<Inan,Nom,Masc,Sing>),\n",
       " DocToken(start=638, stop=639, text=',', id='7_9', head_id='7_11', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=640, stop=641, text='«', id='7_10', head_id='7_11', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=641, stop=646, text='префы', id='7_11', head_id='7_5', rel='conj', pos='NOUN', feats=<Inan,Nom,Fem,Plur>),\n",
       " DocToken(start=646, stop=647, text='»', id='7_12', head_id='7_11', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=648, stop=654, text='Мечела', id='7_13', head_id='7_11', rel='nmod', pos='PROPN', feats=<Inan,Gen,Masc,Sing>),\n",
       " DocToken(start=654, stop=655, text=',', id='7_14', head_id='7_15', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=656, stop=664, text='Татнефти', id='7_15', head_id='7_5', rel='conj', pos='PROPN', feats=<Inan,Gen,Fem,Sing>),\n",
       " DocToken(start=665, stop=666, text='и', id='7_16', head_id='7_17', rel='cc', pos='CCONJ'),\n",
       " DocToken(start=667, stop=682, text='Сургутнефтегаза', id='7_17', head_id='7_13', rel='conj', pos='PROPN', feats=<Inan,Loc,Masc,Sing>),\n",
       " DocToken(start=682, stop=683, text='.', id='7_18', head_id='7_3', rel='punct', pos='PUNCT')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sents[6].tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Через неделю заседание X по ставке, и картина на валютном и долговом рынках прояснится. '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_for_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Через неделю заседание X по ставке, и картина на валютном и долговом рынках прояснится. '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_for_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocToken(stop=8, text='Прогнозы', id='1_1', head_id='1_0', rel='root', pos='NOUN', feats=<Inan,Acc,Masc,Plur>),\n",
       " DocToken(start=9, stop=10, text='и', id='1_2', head_id='1_3', rel='cc', pos='CCONJ'),\n",
       " DocToken(start=11, stop=22, text='комментарии', id='1_3', head_id='1_1', rel='conj', pos='NOUN', feats=<Inan,Nom,Masc,Plur>),\n",
       " DocToken(start=22, stop=23, text='.', id='1_4', head_id='1_1', rel='punct', pos='PUNCT')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sents[0].tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ЦБ'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Прогнозы и комментарии. Коррекция или разворот?  Индекс МосБиржи от двухлетних максимумов ушел в коррекцию. Недельная разворотная свеча может настораживать краткосрочных активных трейдеров, а для инвесторов это возможность усилить позиции в подешевевших активах на перспективу — годовому тренду вверх вряд ли пока что-то угрожает.  Рубль за неделю потерял процент, индекс ОФЗ рухнул на минимумы апреля 2022 г. — валютные барьеры ограждают нацвалюту от девальвации и одновременно давят на котировки облигаций. Через неделю заседание ЦБ по ставке, и картина на валютном и долговом рынках прояснится.  Бумаги в фокусе — ТКС Холдинг и Росбанк, «префы» Мечела, Татнефти и Сургутнефтегаза.  На внешнем контуре умеренная коррекция в индексах США, утренние фьючерсы слабы, азиатские площадки окрашены в ярко красный — факторы указывают на негативный старт европейской пятничной сессии акций.  Оцениваем ближайшие перспективы в утреннем материале: https://bcs-express.ru/novosti-i-analitika/prognozy-i-kommentarii-korrektsiia-ili-razvorot'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocSpan(start=532, stop=534, type='ORG', text='ЦБ', tokens=[...], normal='ЦБ'),\n",
       " DocSpan(start=617, stop=628, type='ORG', text='ТКС Холдинг', tokens=[...], normal='ТКС Холдинг'),\n",
       " DocSpan(start=631, stop=638, type='ORG', text='Росбанк', tokens=[...], normal='Росбанк'),\n",
       " DocSpan(start=648, stop=654, type='ORG', text='Мечела', tokens=[...], normal='Мечела'),\n",
       " DocSpan(start=656, stop=664, type='ORG', text='Татнефти', tokens=[...], normal='Татнефть'),\n",
       " DocSpan(start=667, stop=682, type='ORG', text='Сургутнефтегаза', tokens=[...], normal='Сургутнефтегаза'),\n",
       " DocSpan(start=735, stop=738, type='LOC', text='США', tokens=[...], normal='США')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sents[4].spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Через неделю заседание X по ставке, и картина на валютном и долговом рынках прояснится. '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_for_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.8 ('sentim')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71b43b1161994dabcaec092bbb75f034ad127bcc84e43838e2d97cd0666f702c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
